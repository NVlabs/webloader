{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing and Torchvision\n",
    "\n",
    "PyTorch's torchvision preprocesses images using the PIL image procesing library. WebLoader can easily deal with this; you choose a decoder of type \"PIL\", the use the regular Torch preprocessing, and finally specify `converters=\"torch\"`, which will return Torch tensors for all the elements.\n",
    "\n",
    "Doing significant image processing during data augmentation is compute intensive, so you may want to use multiprocessing to speed this up; the `MultiWebLoader` class can perform this for you. This is similar to the use of multiprocessing in `torch.data`.\n",
    "\n",
    "However, generally, it's a better choice (1) to do data augmentation offline prior to learning (if you have the storage) and (2) parallelize explicitly using the `tensorcom` package (rather than relying on Python `multiprocessing`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!test -f training_ua.tgz || curl http://storage.googleapis.com/lpr-imagenet/imagenet_train-0000.tgz -o training_ua.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_url = \"training_ua.tgz\"\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webloader as wl\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard torchvision-based ImageNet preprocessing\n",
    "\n",
    "def info(x):\n",
    "    print(type(x), x.shape)\n",
    "    return x\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "preproc = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = wl.MultiWebLoader(\n",
    "    source_url, 1000000,\n",
    "    decode=\"PIL\", # decode all image types to PIL\n",
    "    fields=\"ppm;png;jpg cls\", # extract these fields\n",
    "    transforms=[preproc, None], # apply these transformations to the fields\n",
    "    converters=\"torch\", # shorthand for converters=[wl.totorch(), wl.totorch()]\n",
    "    processes=4,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.float32\n",
      "CPU times: user 102 ms, sys: 25 ms, total: 127 ms\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, b in enumerate(training):\n",
    "    if i==0: print(b[0].shape, b[0].dtype)\n",
    "    if i>=100: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = wl.WebLoader(\n",
    "    source_url, 1000000,\n",
    "    decode=\"PIL\", # decode all image types to PIL\n",
    "    fields=\"ppm;png;jpg cls\", # extract these fields\n",
    "    transforms=[preproc, None], # apply these transformations to the fields\n",
    "    converters=\"torch\", # shorthand for converters=[wl.totorch(), wl.totorch()]\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.float32 torch.Size([32]) torch.int64\n",
      "CPU times: user 1min 3s, sys: 437 ms, total: 1min 3s\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, b in enumerate(training):\n",
    "    if i==0: print(b[0].shape, b[0].dtype, b[1].shape, b[1].dtype)\n",
    "    if i>=100: break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
